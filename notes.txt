9/14/2017
FigAccByK

accByK =

    0.7014    0.7825    0.6588    0.5649    0.6468
    0.5679    0.7232    0.7290    0.7467    0.7571
    0.6377    0.6859    0.7525    0.7077    0.8275
    0.4443    0.5217    0.5161    0.4720    0.4699
    0.7365    0.8019    0.7570    0.7293    0.7499
    0.6412    0.7907    0.6973    0.6916    0.7636
    0.6158    0.6721    0.6623    0.6916    0.7123
    0.5285    0.6139    0.7449    0.7918    0.7778


7/25/2017
Running with k=12 for subjects 55,56 we get pretty good MvsR discrimination:
results =

   55.0000    1.0000    1.0000    0.9973    0.8421    0.8162    0.9986    0.9598
   55.0000    1.0000    2.0000    0.6530    0.6879    0.8119    0.8343    0.7947
   55.0000    1.0000    3.0000    0.7715    0.8105    0.8712    0.9020    0.8925
   55.0000    1.0000    4.0000    0.4620    0.7072    0.8413    0.7542    0.9754
   56.0000    2.0000    1.0000    0.9040    0.7072    0.7108    0.9365    0.9948
   56.0000    2.0000    2.0000    0.0368    0.6534    0.7160    0.5205    0.6120
   56.0000    2.0000    3.0000    0.8728    0.7061    0.8353    0.9411    0.9226
   56.0000    2.0000    4.0000    0.8555    0.7828    0.7971    0.9447    0.8896



7/3/2017
Working on the paper  -- here is sample data for a k-means classification
of Sub 55 Session 1

classifier:
 4     2     3     2     1     1     2     3     2     4     1     3

clusters':
    0.5054    0.4643    0.2444    0.1190    0.1743    0.1718    0.3415    0.3159    0.1935    0.6179    0.1326    0.2456
    0.2460    0.1897    0.0867    0.1108    0.1199    0.0760    0.2209    0.1255    0.1937    0.3084    0.1231    0.2016
    0.2723    0.1699    0.1717    0.1290    0.1707    0.0759    0.2720    0.1310    0.2485    0.3920    0.1862    0.2228
    0.4619    0.4669    0.2720    0.1569    0.2162    0.2357    0.3931    0.3355    0.2926    0.6198    0.1634    0.2632
    0.1691    0.1471    0.2199    0.0924    0.1248    0.1735    0.1681    0.2025    0.1289    0.1196    0.3262    0.3031
    0.2100    0.1468    0.0725    0.0699    0.0762    0.0620    0.1466    0.2094    0.1546    0.1830    0.0915    0.2208
    0.2312    0.1226    0.2066    0.0911    0.0747    0.0532    0.1412    0.2755    0.1769    0.1772    0.1551    0.2503
    0.2000    0.1665    0.2434    0.1276    0.1546    0.2425    0.1684    0.2093    0.1787    0.1279    0.3280    0.3046
    0.1200    0.1838    0.2057    0.5579    0.1364    0.2716    0.1420    0.1788    0.4689    0.0880    0.1378    0.1055
    0.2462    0.3976    0.5401    0.5801    0.3152    0.6001    0.2374    0.1769    0.3478    0.2211    0.4832    0.2102
    0.2366    0.5139    0.2330    0.5558    0.1942    0.7066    0.2294    0.1268    0.2886    0.2048    0.3712    0.2006
    0.1060    0.1489    0.1604    0.4636    0.1000    0.1707    0.1113    0.1502    0.2750    0.0763    0.0983    0.0874
    0.0793    0.0697    0.1250    0.0750    0.0712    0.1082    0.0743    0.1085    0.0837    0.0435    0.3042    0.2535
    0.1464    0.1139    0.0954    0.0571    0.0648    0.0614    0.1043    0.3656    0.1456    0.1153    0.0898    0.2344
    0.0964    0.0654    0.2163    0.0662    0.0423    0.0370    0.0608    0.3634    0.1352    0.0623    0.1167    0.1937
    0.0953    0.0795    0.1247    0.1181    0.0880    0.1541    0.0801    0.1130    0.1280    0.0436    0.3221    0.2500
    0.1262    0.1350    0.2050    0.1557    0.4933    0.2749    0.2741    0.1944    0.1250    0.1310    0.0992    0.0922
    0.1514    0.1519    0.2053    0.1821    0.4240    0.2005    0.2909    0.1226    0.1582    0.1722    0.2125    0.1330
    0.1634    0.1282    0.1724    0.1579    0.5180    0.1273    0.2966    0.1033    0.1508    0.1637    0.1708    0.1325
    0.1368    0.1383    0.1995    0.1337    0.4412    0.1970    0.2472    0.1920    0.1257    0.1324    0.0882    0.0948

IMAGES:
sub55s1Plot
sub55s1Classification
sub55s1Accuracy

6/29/2017
Training and testing on all 4 sessions of all 10 subjects with label = subject id (from 1 to 10 we get 73% accuracy ...

       32572           0        1231        1545        3436           8         200        2410        1961         302
           5       44924         833        2028           0        4529        5637        1365         211        2090
        1361          51       43394        1072        3376        4525        3934        1334        1268           0
        3305         644        1161       39247           0        1448        3713         634           0        4043
        7894         162         529           3       39987        1022         610         279        4338        4103
           0         113         196           0          39       30543        2691         831        4648           0
         711         213           0         733           0         301       24846           0        2548           0
         313        1385         423           0         525        1463        1287       37670        2969        2419
          23           0           0           0          42        1653         962         295       25317         340
        1816         106         233        3372         595        2192        4119        3182        4740       34703

and looking at these as percentages for each subject we get
  67.8583         0    2.5646    3.2188    7.1583    0.0168    0.4167    5.0208    4.0854    0.6292
    0.0104   94.3821    1.7354    4.2250         0    9.4979   11.7440    2.8438    0.4396    4.3542
    2.8354    0.1071   90.4042    2.2333    7.0333    9.4896    8.1960    2.7792    2.6417         0
    6.8854    1.3530    2.4188   81.7646         0    3.0367    7.7356    1.3208         0    8.4229
   16.4458    0.3404    1.1021    0.0063   83.3063    2.1433    1.2709    0.5813    9.0375    8.5479
         0    0.2374    0.4083         0    0.0813   64.0529    5.6064    1.7313    9.6833         0
    1.4812    0.4475         0    1.5271         0    0.6312   51.7636         0    5.3083         0
    0.6521    2.9098    0.8812         0    1.0938    3.0681    2.6813   78.4792    6.1854    5.0396
    0.0479         0         0         0    0.0875    3.4666    2.0042    0.6146   52.7437    0.7083
    3.7833    0.2227    0.4854    7.0250    1.2396    4.5969    8.5814    6.6292    9.8750   72.2979

We get better results by throwing away the bad data and looking at session 1 for subjects [55,56,57,58,59,61,71,72];

So we get 96% accuracy overall and with a range from 88 to 100% for different subjects

  88.7750         0         0         0         0         0         0         0
    0.4250   99.4403         0         0         0         0         0         0
    2.8417    0.5597   99.9750         0    0.7083         0    1.6250         0
    7.7750         0    0.0250  100.0000    0.4833         0    0.7500         0
         0         0         0         0   96.9167         0    3.8000         0
         0         0         0         0    0.7000   98.4370    2.5667         0
    0.1833         0         0         0    1.1917    1.5630   88.8500         0
         0         0         0         0         0         0    2.4083  100.0000


accByLabel1 =

       10653           0           0           0           0           0           0           0
          51       11549           0           0           0           0           0           0
         341          65       11997           0          85           0         195           0
         933           0           3       12000          58           0          90           0
           0           0           0           0       11630           0         456           0
           0           0           0           0          84       11777         308           0
          22           0           0           0         143         187       10662           0
           0           0           0           0           0           0         289       12000

>> A = accByLabel1;
>> s = 0;
>> for i=[1:length(A)]
s = s+A(i,i);
end
>> s/sum(sum(A))*100

ans =

   96.5369

>> A./sum(A)*100

========
if we try to do 4 fold crossover validation of subject identification we get a 54% accuracy (when random guessing would give 12.5%)
AA =

       25353         153         971        2780        3260         175        3145        3847
        1028       33656        2587       12147        1208        8487        2480       13663
        7237        7992       41273         992        6093        5644        2277        1635
       11334        4561        2961       30566         926        3594        1645        6154
        1359         238           0          88       29127        2902        3109        2518
           0         998           0          90        1505        8722        9903         398
        1689           0         208         129        4981       17058       20755        2063
           0           0           0        1208         900        1102        4686       17722

>> s=0;
>> for i=[1:8]
s = s+AA(i,i);
end
>> s/sum(sum(AA))

ans =

    0.5405

>> AA./sum(AA)

ans =

    0.5282    0.0032    0.0202    0.0579    0.0679    0.0037    0.0655    0.0801
    0.0214    0.7071    0.0539    0.2531    0.0252    0.1780    0.0517    0.2846
    0.1508    0.1679    0.8599    0.0207    0.1269    0.1184    0.0474    0.0341
    0.2361    0.0958    0.0617    0.6368    0.0193    0.0754    0.0343    0.1282
    0.0283    0.0050         0    0.0018    0.6068    0.0609    0.0648    0.0525
         0    0.0210         0    0.0019    0.0314    0.1829    0.2063    0.0083
    0.0352         0    0.0043    0.0027    0.1038    0.3577    0.4324    0.0430
         0         0         0    0.0252    0.0187    0.0231    0.0976    0.3692

=======================




6/23/2017
All Subjects All Sessions 10-fold crossover 
Here is the table for predictions

    0.8777    0.5322    0.7708    0.8816    0.6016    0.8215    0.7201    0.6479    0.9607    0.6462
    0.8781    0.5029    0.8041    0.8179    0.7367    0.7812    0.7087    0.7383    0.9470    0.6780
    0.8998    0.5380    0.7743    0.8695    0.7309    0.8319    0.7227    0.7210    0.9591    0.6360
    0.8984    0.6324    0.8182    0.8319    0.7289    0.7783    0.7632    0.7612    0.9554    0.6459
where the rows correspond to N=1,4,16,64
and the columns correspond to the subject. N has very little effect on the mean accuracy
Here are the means of the 4 rows
    0.7460
    0.7593
    0.7683
    0.7814
k has a bigger effect,

Here is the prediction accuracies for k varying across these values [5,10,20,40,80,150,300,600]

    0.8473    0.5568    0.6937    0.8122    0.5327    0.6331    0.7265    0.5803    0.8513    0.5292
    0.6865    0.6521    0.7404    0.8791    0.7715    0.6837    0.7692    0.7681    0.9230    0.5814
    0.9056    0.7809    0.7554    0.7997    0.5490    0.7208    0.7221    0.7537    0.9481    0.6154
    0.9078    0.7876    0.7506    0.8737    0.7474    0.7536    0.7442    0.5903    0.9475    0.6689
    0.9290    0.6702    0.6128    0.8557    0.5807    0.7560    0.7718    0.5996    0.9599    0.6805
    0.9177    0.7268    0.8179    0.8557    0.5926    0.8037    0.7835    0.6547    0.9617    0.6673
    0.8977    0.7485    0.7182    0.8344    0.5505    0.7836    0.8021    0.6002    0.9597    0.6997
    0.8874    0.8138    0.7634    0.8318    0.6249    0.8463    0.7645    0.5864    0.9612    0.6692
where each row is a different k (from 5 to 600) and the columns are the subjects (from 1 to 10)
Here are the means

    0.6763
    0.7455
    0.7551
    0.7772
    0.7416
    0.7782
    0.7595
    0.7749
It seems that some subjects are much easier and others are much harder to predict than others.
It could also be that those that are hard to predict are just hard to represent using kmeans!
We can do some testing to find out....

Here are the means of the 10 columns which ranges from low 60s for subjects 5,8,10 to mid 90s for subject 9
   0.8724    0.7171    0.7316    0.8428    0.6187    0.7476    0.7605    0.6417    0.9390    0.6389

6/12/2017 
Lets call the classifier kMSW for k-Means with a Sliding Window

Two main results are -- 
* effectiveness of kMSW for Single Subject 4-fold crossover prediction
* effectiveness? of kMSW for Multiple Subject 10-fold crossover prediction

For the latter, our best bet would be to try to classify M vs S and see if
we can get significantly above 50%



6/3/2017
Overview of prediction accuracy for W=600
labels           k=150         k=600      random    95% CI

M-S-R-O          55.9          57.9       25.0      [24.6,25.4]

M-S-R            57.7          65.0       33.0

R-O              62.5          74.0       50.0  *(DAYDREAMING!!)

M-R              65.0          65.0       50.0      [49.3,50.7]

M-O              66.5          77.5       50.0

S-O              71.0          77.0       50.0

MR-S             74.5          84.5       50.0

MR-SO            78.0          84.6       50.0

S-R              78.5          85.5       50.0

M-S              89.5          89.0       50.0













Looking at mean window accuracy by activity for k=600 and W=600 we get
the following with 57.94% accuracy

    0.6560    0.0688    0.3251    0.1212
    0.0378    0.7063    0.0614    0.1409
    0.1841    0.0306    0.3997    0.1821
    0.1221    0.1943    0.2138    0.5557

If we use the kmeans3 version we get an accuracy of 55.13%

    0.5569    0.0358    0.2563    0.0949
    0.0316    0.6888    0.0489    0.1660
    0.2257    0.0446    0.4516    0.2311
    0.1858    0.2308    0.2432    0.5080

The accuracy is a little less, but it is more uniform in
its predictions. This took 12 minutes to compute for multiple k and W=600



  If we collapse into Problem Solving (Math+Read) and Relaxation (Shut+Open)
  we get

    0.8910    0.1990
    0.1090    0.8010

  which shows that on the average over all 36 session it correctly predicts
  89% of the Problem Solving activity and
  80% of the Relaxation.
  So focusing on PS, there are 11% false negatives and 20% false positives
  The general accuracy rate is therefor 85% and the error rate is 15%. There is
  however a very large variance...

  If we look at k=150 instead we get

    0.6090    0.0484    0.3408    0.1621
    0.0437    0.6639    0.0797    0.1167
    0.2043    0.0393    0.3587    0.1163
    0.1430    0.2484    0.2207    0.6050
  Observe the connection between MR and SO
  Almost all of the M predictions are in M or R (94%)
  and the M+R account for 81.3% of the M predictions.
  The weakest prediction of all is Reading
    it only gets 36% accuracy and accounts for 20% of the false negatives in M
    Likewise, M accounts for 34% of the false positives in Reading...

Again, using kmeans3 for k=160 (it needs to be a multiple of 4 to work well)
we get 56.93% accuracy
   0.6154    0.0364    0.3007    0.0973
   0.0239    0.6990    0.0406    0.1495
   0.1681    0.0570    0.4163    0.2068
   0.1926    0.2077    0.2424    0.5464
and it does a slightly better job in reading ...

Looking at all but Open for k=150 we get accuracy of 57.7
58          16          41
11          71          14
30          12          44
with k=600 we get accuracy = 65.0
64           8          38
 6          84          15
30           8          47



  Looking at PS vs RX we get 78% accuracy!
    0.7564    0.1830
    0.2436    0.8170

Now lets look at Math vs Reading ..
at k=150 W=600 we get an average accuracy of 65%
     66          35
     34          65

at k=600 W=600 we get again about 65% accuracy
     61          30
     39          70

Next looking at Math vs Shut we get an accuracy around 90%

for k=600 we get 89.0%
        93          15
         7          85
For k=150 we get 89.5%
        93          14
         7          86

Looking at MR-S we get 74.5% accuracy for k=150
97          48
 3          52
The MR get almost 100% of the MR activity,
but it also creates false positives at a rate of almost 50% for S

With k=600 it does better with 85.5% accuracy
95          24
 5          76
again the MR is almost perfect on MR, but it generates 24% false positives in S

S vs O k=150 gets 71% accuracy
69          27
31          73
for k=600 gets  77% accuracy
71          17
29          83

M vs O with k=150 accuracy = 66.5%
74          37
26          63
with k=600 accuracy = 75.5%
73          22
27          78

S vs R with k=150, accuracy = 78.5%
86          29
14          71
with k=300, accuracy = 85.5%
86          15
14          85

R vs O with k=150, accuracy = 62.5%
68          43
32          57
with k=600, accuracy = 74%
77          29
23          71



5/31/2017

Lets create the first graph, that shows how accuracy varies for single subject single session
kmeans classification as k varies with W=600 and N=1, and lets just use the minaccuracy
so we see what the accuracy is for each of the four activities and take the minimum...


5/29/2017

What measures of accuracy
1) Draw the overview plots ... gives visual representation of accuracy
2) Calculate the prediction accuracy per activity and get 4 probabilities
    take min and take mean, median for those probabilities...


What to classify?
1) MSRO do all 4 classifications ...
2) pairwise -- just try to classify MvS or MvR or SvO or .. MRvSO
3) others are classify by subject id (try to recognize different subjects)
4) try to classify by session (probably not for good ...)
We can do
MvSvRvO
MvS


Within Subject Classification
How good is kmeans at representing an activity map, i.e. predicting activity classifications?
and how does the accuracy vary based on k, W, N
3 graphs for MvSvRvO
   e.g. try baseline of k=120 W=600 N=5, then
       let k vary from say 4,12,48,120,600,1200 and
       for each k we get 36 accuracies (one for each session)
       can use a box and whisker plot to show the effect of k on accuracy ...
       Same for W, say with W=1,10,100,300,600,1200,3000
       Same for N=1,2,4,8,16...
3 graphs for MvS

How regular are the brainwaves for a single subject across multiple sessions.
Use 4-fold crossover validation. Train on 3, predict on 4th, average results...
using best k,W,N from previous...
Show all 36 prediction graphs (based on training on the other three...)
1 plot for each MVSR and MS

Between Subject Classification
How good is kmeans at representing an activity map for all 9 subjects....
Use one classifier for all 9 subjects and see how the accuracy varies based on
3 graphs for MvSvRvO  (k varies, W varies, N varies)
3 for MvS

How regular are the brainwave for different subjects across all sessions.
Use 9-fold crossover validation. Train on 8 subjects (all sessions), test on 9th subject, average.
Show the prediction graphs....
2 plots ...





The next thing I need to do is to describe in detail the kinds of analysis
we need to do. The key ideas are:
1) discuss measures of accuracy for a k-means classifier
2) discuss the parameters used to tune k-means
    k
    N
    W
    c
3) show how accuracy varies based on different choices of the k-means parameters

Another issue to discuss is what we choose to classify, e.g.

Math vs Shut
 We can get 98% mean window accuracy for k=120 W=600 N=3

Math vs Read vs Shut vs Open
We can get 82% min win accuracy for k=120 W=600 (and 97% mean win accuracy)
